{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorization Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn import datasets, model_selection, metrics\n",
    "from sklearn import linear_model, naive_bayes, tree\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Importing the Data\n",
    "df = pd.read_csv(\"../CDS303/CSV Files/Post_EDA_encoded_df.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization (or scaling) of data is a crucial preprocessing step. It does not change the underlying relationships between the features, but it scales them for comparability. I was having issues with convergence until the data was scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Scale the data\n",
    "X = df.drop(['Safe_Email'], axis=1)\n",
    "y = df['Safe_Email']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified K-Fold due to imbalanced classes\n",
    "\n",
    "\n",
    "# Splitting the Data, Training versus Testing\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_scaled, y, test_size=0.3)\n",
    "\n",
    "X_train_pct = round((len(X_train) / (len(X_train) + len(X_test))), 2)\n",
    "X_test_pct = round((len(X_test) / (len(X_train) + len(X_test))), 2)\n",
    "\n",
    "print('Classification training dataset shape is:', X_train.shape, 'Testing dataset shape is:', X_test.shape)\n",
    "print('Regression train/test split is:', X_train_pct, '/', X_test_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train,y_train)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models From the Categorization Example on Blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "param_dict = {\n",
    "    'knn': ('n_neighbors', [2, 3, 4, 5, 6, 7, 8], 'weights', ['uniform', 'distance']),\n",
    "    'svc': ('C', [1, 2], 'kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'dt': ('criterion', ['gini', 'entropy', 'log_loss'], 'max_depth', [2, 3, 4, 5, 6]),\n",
    "    'rf': ('n_estimators', [50, 100, 150, 200], 'max_depth', [2, 3, 4, 5, 6]),\n",
    "    'mlp': ('activation', ['identity', 'logistic', 'tanh', 'relu'], 'solver', ['lbfgs', 'sgd', 'adam']),\n",
    "    'lrm': ('penalty', ['l2'], 'C', [0.1, 1, 5, 10], 'solver', ['liblinear', 'newton-cg'])\n",
    "}\n",
    "\n",
    "model_dict = {\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'svc': SVC(),\n",
    "    'dt': DecisionTreeClassifier(),\n",
    "    'rf': RandomForestClassifier(),\n",
    "    'mlp': MLPClassifier(),\n",
    "    'lrm': linear_model.LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "summary = []\n",
    "cm_df = pd.DataFrame()\n",
    "\n",
    "for i in ['knn', 'svc', 'dt', 'rf', 'mlp','lrm']:\n",
    "    param_grid = list(itertools.product(param_dict[i][1], param_dict[i][3]))\n",
    "    for k in range(len(param_grid)):\n",
    "        param_grid_dict = {param_dict[i][0]: param_grid[k][0], param_dict[i][2]: param_grid[k][1]}\n",
    "        model = model_dict[i]\n",
    "        \n",
    "        for train_index, test_index in skf.split(X_scaled, y):\n",
    "            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            model.set_params(**param_grid_dict)\n",
    "            clf = model.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            summary.append([i, param_dict[i][0], param_grid[k][0], param_dict[i][2], param_grid[k][1],\n",
    "                            accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'),\n",
    "                            recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')])\n",
    "            \n",
    "            dfpred = pd.DataFrame([y_pred]).transpose()\n",
    "            dftest = pd.DataFrame([y_test]).transpose()\n",
    "            cm_data_temp = pd.concat([dfpred, dftest], axis=1)\n",
    "            cm_df = pd.concat([cm_df, cm_data_temp], axis=0)\n",
    "\n",
    "summary_df = pd.DataFrame(summary, columns=('model', 'param1', 'parval1', 'param2', 'parval2', 'accuracy', 'precision', 'recall', 'f1'))\n",
    "best_df = summary_df.nlargest(15, 'f1')\n",
    "cm_df.columns = ('ypred', 'ytest')\n",
    "\n",
    "best_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = cm_df['ytest'].isna().sum()\n",
    "print(\"Number of missing values in 'ytest' column:\", missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingValuesNaN = round(1057040/cm_df.shape[0]*100,4)\n",
    "print(\"Unfortunately,\", missingValuesNaN, \"percent of the models did not converge.\")\n",
    "# Drop rows with NaN values in 'ytest' column\n",
    "cm_df = cm_df.dropna(subset=['ytest','ypred'])\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "cm_df = cm_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix = confusion_matrix(cm_df['ytest'], cm_df['ypred'])\n",
    "print(cm_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "ax = sns.heatmap(cm_matrix, annot=True, cmap='Greens', fmt='.2f')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['0','1'])\n",
    "ax.yaxis.set_ticklabels(['0','1'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our model is more likely to give a false negative than a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "In our analysis and preperation of the dataset there was an implicit assumption that features like special characters and the presence of a URL will be good predictors of phishing and spam emails. \n",
    "\n",
    "### Results\n",
    "No matter what modeling technique is used the results are disappointing.\n",
    "The best model has an accuracy of 0.851381,precision of\t0.845900, and a recall of\t0.837787. A quick glance at the confusion matrix will make clear that false positives and negatives are common. Another clear indicator of this failure is that 10 of the 15 best models of all of the possible models we tried were \"dumb learning\" k nearest neighbor models.\n",
    "\n",
    "### Comparison to NLP Approach\n",
    "The Natural Language Processing technique of tokenisation (seperating the email text into key words) followed by vectorization (assigning numbers to those tokens) provides a much better foundation for applying machine learning techniques. The following section applies this approach to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../CDS303/CSV Files/Sentiment_Analysis_Export_df.csv\")\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fitting the vectorizer to the text\n",
    "tfidf_vectorizer.fit(df[\"Tokenized_Text\"])\n",
    "\n",
    "# Transforming tokenized text into TF-IDF vectors\n",
    "tfidf_vectors = tfidf_vectorizer.transform(df[\"Tokenized_Text\"])\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fitting the logistic regression model, \"Email Type is the target variable\"\n",
    "logistic_model.fit(tfidf_vectors, df[\"Email Type\"])\n",
    "\n",
    "\n",
    "# Define the selected configurations\n",
    "selected_configurations = [\n",
    "    {'penalty': 'l1', 'C': 5.0, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l1', 'C': 0.5, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 5.0, 'solver': 'liblinear'},\n",
    "    {'penalty': 'l2', 'C': 10.0, 'solver': 'newton-cg'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize an empty list to store confusion matrices\n",
    "conf_matrices = []\n",
    "\n",
    "summary = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for config in selected_configurations:\n",
    "    for train_index, test_index in skf.split(tfidf_vectors, df[\"Email Type\"]):\n",
    "        X_train, X_test = tfidf_vectors[train_index], tfidf_vectors[test_index]\n",
    "        y_train, y_test = df[\"Email Type\"][train_index], df[\"Email Type\"][test_index]\n",
    "        \n",
    "        model = LogisticRegression(**config)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        conf_matrices.append(conf_matrix)\n",
    "\n",
    "        # Store the results as a dictionary\n",
    "        result = {\n",
    "            'Param1': config['penalty'],\n",
    "            'Parval1': config['C'],\n",
    "            'Param2': 'solver',\n",
    "            'Parval2': config['solver'],\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1\n",
    "        }\n",
    "        summary.append(result)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "summary_df = pd.DataFrame(summary, columns=('Param1', 'Parval1', 'Param2', 'Parval2', 'Accuracy', 'Precision', 'Recall', 'F1'))\n",
    "\n",
    "# Compute average confusion matrix\n",
    "avg_conf_matrix = np.mean(conf_matrices, axis=0)\n",
    "\n",
    "# Display the average confusion matrix\n",
    "print(\"Average Confusion Matrix:\")\n",
    "print(avg_conf_matrix)\n",
    "\n",
    "# Display the results\n",
    "best_nlpDF = summary_df.nlargest(5, \"F1\")\n",
    "print(best_nlpDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nlpDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing seaborn again beacuse the other code block takes an hour to run\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "ax = sns.heatmap(avg_conf_matrix, annot=True, cmap='Greens', fmt='.2f')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['0','1'])\n",
    "ax.yaxis.set_ticklabels(['0','1'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>Safe Email</td>\n",
       "      <td>['6', '1100', 'disc', 'uniformitarianism', '10...</td>\n",
       "      <td>0.9798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>Safe Email</td>\n",
       "      <td>['side', 'galicismos', 'galicismo', 'spanish',...</td>\n",
       "      <td>0.4329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>Safe Email</td>\n",
       "      <td>['equistar', 'deal', 'ticket', 'still', 'avail...</td>\n",
       "      <td>0.8591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>Phishing Email</td>\n",
       "      <td>['hello', 'hot', 'lil', 'horny', 'toy', 'one',...</td>\n",
       "      <td>0.9640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>Phishing Email</td>\n",
       "      <td>['software', 'incredibly', 'low', 'price', '86...</td>\n",
       "      <td>0.0534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Email Text      Email Type  \\\n",
       "0  re : 6 . 1100 , disc : uniformitarianism , re ...      Safe Email   \n",
       "1  the other side of * galicismos * * galicismo *...      Safe Email   \n",
       "2  re : equistar deal tickets are you still avail...      Safe Email   \n",
       "3  \\nHello I am your hot lil horny toy.\\n    I am...  Phishing Email   \n",
       "4  software at incredibly low prices ( 86 % lower...  Phishing Email   \n",
       "\n",
       "                                      Tokenized_Text  Sentiment_Score  \n",
       "0  ['6', '1100', 'disc', 'uniformitarianism', '10...           0.9798  \n",
       "1  ['side', 'galicismos', 'galicismo', 'spanish',...           0.4329  \n",
       "2  ['equistar', 'deal', 'ticket', 'still', 'avail...           0.8591  \n",
       "3  ['hello', 'hot', 'lil', 'horny', 'toy', 'one',...           0.9640  \n",
       "4  ['software', 'incredibly', 'low', 'price', '86...           0.0534  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "X = tfidf_vectorizer.fit_transform(df['Email Text'])\n",
    "#target\n",
    "Y = df[\"Email Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing again because OG code block with these imports take over an hour to run\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Multi-Layer Perceptron (MLP) Classifier\": MLPClassifier(),\n",
    "    \"Support Vector Classification (SVC)\": SVC()\n",
    "}\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, X,Y,CV_Folds=5):\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    # precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    # recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    # f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "    cv_results = cross_validate(model, X, Y, cv=CV_Folds, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "    \n",
    "    # Calculate the mean of each metric across all folds\n",
    "    accuracy = cv_results['test_accuracy'].mean()\n",
    "    precision = cv_results['test_precision_weighted'].mean()\n",
    "    recall = cv_results['test_recall_weighted'].mean()\n",
    "    f1 = cv_results['test_f1_weighted'].mean()\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs for around hours on my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      " Accuracy: 0.9684, Precision: 0.9686, Recall: 0.9684, F1 Score: 0.9683\n",
      "\n",
      "Decision Tree Classifier:\n",
      " Accuracy: 0.9187, Precision: 0.9188, Recall: 0.9187, F1 Score: 0.9187\n",
      "\n",
      "K-Nearest Neighbors:\n",
      " Accuracy: 0.7656, Precision: 0.8792, Recall: 0.7656, F1 Score: 0.7378\n",
      "\n",
      "Multi-Layer Perceptron (MLP) Classifier:\n",
      " Accuracy: 0.9886, Precision: 0.9886, Recall: 0.9886, F1 Score: 0.9886\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(53417) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(53418) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(53419) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(53420) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(53421) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(53422) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classification (SVC):\n",
      " Accuracy: 0.9856, Precision: 0.9857, Recall: 0.9856, F1 Score: 0.9856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over models and print their scores\n",
    "for name, model in models.items():\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, X,Y,CV_Folds=5)\n",
    "    print(f\"{name}:\\n Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to print confusion matrices of each of the classifier models\n",
    "# to compare effectivesness against chosen logistic regression method\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix_for_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,model in models.items():\n",
    "    plot_confusion_matrix_for_model(model, X_train, X_test, y_train, y_test, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
